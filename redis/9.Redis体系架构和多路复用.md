# Redis体系架构和io多路复用

## Redis的体系架构

Redis利用epoll来实现IO多路复用，**将连接信息和事件放到队列中**，一次放到文件事件**分派器**，事件分派器将事件分发给事件**处理器**

![2023-02-14-08-37-07-image.png](C:\Users\sugon\Desktop\study_note\images\3c93fe73f39d4f67c9fd07d167436e8d5500d31d.png)

**如果没有io多路复用会怎么样？**

Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以I/O操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现

**什么是io多路复用？**

所谓I/O多路复用机制，就是说通过一种机制，可以监视多个描述符，一旦某个描述符就绪(一般是读就绪或写就绪)，能够通知程序进行相应的读写操作。这种机制的使用需要 select、 poll、epoll来配合。多个连接共用一个阻塞对象，应用程序只需要在一个阻塞对象上等待，无需阻塞等待所有连接。当某条连接有新的数据可以处理时，操作系统通知应用程序，线程从阻塞状态返回，开始进行业务处理。

Redis 服务采用 Reactor 的方式来实现文件事件处理器(每一个网络连接其实都对应一个文件描述符)

Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器。它的组成结构为4部分:

* **多个套接字**

* **lO多路复用程序**

* **文件事件分派器**

* **事件处理器**

**因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型**

**结论**

Redis 6.0 将**网络数据读写、请求协议解析**通过多个IO线程的来处理（多线程）

对于真正的**命令执行**来说，仍然使用主线程操作（单线程）

![2023-02-14-09-03-41-image.png](C:\Users\sugon\Desktop\study_note\images\a3120bfef5c86d732139cc1da86a63f57362144a.png)

## IO多路复用前置知识

### 是什么

**I/0**:网络I/0

**多路**:多个客户端连接(连接就是套接字描述符，即 socket 或者 channel)
**复用**:复用一个或几个线程。也就是说一个或一组线程处理多个 TCP连接,使用单进程就能够实现同时处理多个客户端的连接

**一句话：**

一个服务端进程可以同时处理多个套接字描述符。
其发展可以分select->poll->epoll三个阶段来描述。

### 同步与异步

**同步**：调用者要**一直等待**调用结果的通知后才能进行后续的执行，现在就要，我可以等，等出结果为止

**异步**：指被调用方先返回应答让调用者先回去，然后再计算调用结果，计算完最终结果后再通知并返回给调用方，异步调用要想获得结果一般通过**回调**

**总结：**

同步、异步的讨论对象是被调用者(服务提供者)，重点在于获得调用结果的消息通知方式上

### 阻塞与非阻塞

**阻塞**：调用方一直在等待而且别的事情什么都不做，当前进/线程会被挂起，啥都不干
**非阻塞**：调用在发出去后，调用方先去忙别的事情，不会阻塞当前进/线程，而会立即返回

**总结：**

阻塞、非阻塞的讨论对象是调用者(服务请求者)，重点在于等消息时候的行为，调用者是否能干其它事

**总结**：**异步非阻塞**是我们最喜欢的模式，调用者和服务提供者都不需要等待。

## BIO和NIO

### BIO

![2023-02-14-09-26-24-image.png](C:\Users\sugon\Desktop\study_note\images\edbdee1efc497b8f8304f9869c912ade39ad0e41.png)

当用户进程调用了recvfrom这个系统调用，kernel就开始了io的第一个阶段:准备数据 (对于网络io来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来),这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞(当然，是进程自己选择的阻塞) 。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。

所以，**BIO的特点就是在IO执行的两个阶段都被block了。**

### 多线程BIO

虽然可以解决客户端线程等待的问题，但是由于每来一个客户端就要开一条线程，导致系统资源占用大，性能开销大

**问题**：

每来一个客户端，就要开辟一个线程，如果来1万个客户端，那就要开辟1万个线程。在操作系统中用户态不能直接开辟线程，需要调用内核来创建的一个线程，这其中还涉及到用户状态的切换(上下文的切换) ，十分耗资源。

**解决方法**：

**第一个办法: 使用线程池**
这个在客户端连接少的情况下可以使用，但是用户量大的情况下，你不知道线程池要多大，太大了内存可能不够，也不可行。

**第二个办法: NIO(非阻塞式IO) 方式**
因为read()方法堵塞了，所有要开辟多个线程，如果什么方法能使read()方法不堵塞，这样就不用开辟多个线程了，这就用到了另一个IO模型，NIO(非阻塞式IO)

## NIO

### NIO是什么

在**NIO模式中，一切都是非阻塞的**:
accept()方法是非阻塞的，如果没有客户端连接，就返回error

read()方法是非阻塞的，如果read()方法读取不到数据就返回error，如果读取到数据时只阻塞read()方法读数据的时间
在NIO模式中，只有一个线程:
当一个客户端与服务端进行连接，这个socket就会加入到一个数组中，隔一段时间遍历一次看这个socket的read()方法能否读到数据，这样**一个线程就能处理多个客户端的连接和读取了**

![](C:\Users\sugon\Desktop\study_note\images\2023-02-14-14-43-15-image.png)

NIO成功的解决了BIO需要开启多线程的问题，NIO中一个线程就能解决多个socket，但是还存在2个问题。

### 优点和缺点

**问题1**

这个模型在客户端少的时候十分好用，但是客户端如果很多，比如有1万个客户端进行连接，那么每次循环就要遍历1万个socket，如果一万个socket中只有10个socket有数据，也会遍历一万个socket，就会做很多无用功，每次遍历遇到 read 返回 -1 时仍然是一次浪费资源的系统调用。

**问题2**

而且这个遍历过程是在用户态进行的，用户态判断socket是否有数据还是调用内核的read()方法实现的，这就涉及到用户态和内核态的切换，每遍万一个就要切换一次，开销很大因为这些问题的存在。

**优点**:不会阻塞在内核的等待数据过程，每次发起的 I/O 请求可以立即返回，不用阻塞等待，实时性较好。

**缺点**:轮询将会不断地询问内核，这将占用大量的 CPU 时间，系统资源利用率较低，所以一般web 服务器不使用这种 IO模型。

### 结论

让Linux内核搞定上述需求，我们将一批文件描述符通过一次系统调用传给内核由内核层去遍历，才能真正解决这个问题。**IO多路复用应运而生，也即将上述工作直接放进Linux内核，不再两态转换而是直接从内核获得结果，因为内核是非阻塞的。**

## IO多路复用与反应模式

### IO多路复用

![](C:\Users\sugon\Desktop\study_note\images\2023-02-14-15-09-02-image.png)

多路复用快的原因在于，操作系统提供了这样的系统调用，使得原来的 while 循环里多次系统调用变成了**一次系统调用 + 内核层遍历这些文件描述符**。

所谓 /O 多路复用机制，就是说通过一种机制，可以监视多个描述符，一旦某个描述符就绪(一般是读就绪或写就绪)，能够通知程序进行相应的读写操作。

这种机制的使用需要 select 、 poll 、epoll来配合。多个连接共用一个阻塞对象，应用程序只需要在一个阻塞对象上等待，无需阻塞等待所有连接。当某条连接有新的数据可以处理时，操作系统通知应用程序，线程从阻塞状态返回，开始进行业务处理。

### reactor反应模式

Reactor 模式也叫 Dispatcher 模式，**即 I/O 多了复用统一监听事件**，收到事件后分发(Dispatch 给某进程)，是编写高性能网络服务器的必备技术。

将用户socket对应的fd注册进epoll。然后epoll帮你监听哪些socket上有消息到达，这样就避免了大量的无用操作。此时的socket应该采用非阻塞模式。这样，整个过程只在调用select、poll、epoll这些调用的时候才会阻塞，收发客户消息是不会阻塞的，整个进程或者线程就被充分利用起来，这就是**事件驱动**，所谓的**reactor反应模式**

![](C:\Users\sugon\Desktop\study_note\images\2023-02-14-15-33-16-image.png)

![](C:\Users\sugon\Desktop\study_note\images\2023-02-14-15-41-16-image.png)

### Select

**执行流程**

分析select函数的执行流程

1.select是一个阻塞函数，当没有数据时，会一直阻塞在select那一行。
2.当有数据时会将rset中对应的那一位置为1
3.select函数返回，不再阻塞
4.遍历文件描述符数组，判断哪个fd被置位了
5.读取数据，然后处理

**缺点**

* bitmap最大1024位，一个进程最多只能处理1024个客户端

* &rset不可重用，每次socket有数据就相应的位会被置位

* 文件描述符数组拷贝到了内核态(只不过无系统调用切换上下文的开销。 (内核层可优化为异步事件通知) )，仍然有开销。select 调用需要传入 fd 数组，需要拷贝一份到内核，高并发场景下这样的拷贝消耗的资源是惊人的。(可优化为不复制)

* select并没有通知用户态哪一个socket有数据，仍然需要O(n)的遍历。select 仅仅返回可读文件描述符的个数，具体哪个可读还是要用户自己遍历。(可优化为只返回给用户就绪的文件描述符，无需用户做无效的遍历)

**小总结**

select方式，既做到了一个线程处理多个客户端连接(文件描述符)，又减少了系统调用的开销(多个文件描述符只有一次 select 的系统调用 + N次就绪状态的文件描述符的 read 系统调用

### poll

**poll的执行流程:**

1.将五个fd从用户态拷贝到内核态

2.poll为阻塞方法，执行poll方法，如果有数据会将fd对应的revents置为POLLIN
3poll方法返回
4.循环遍历，查找哪个fd被置位为POLLIN了
5.将revents重置为0便于复用
6.对置位的fd进行读取和处理

**解决的问题:**

1.解决了bitmap大小限制

2.解决了rset不可重用的情况

后面由于二者原理相同，所以没能解决

**优点**

1、poll使用polfd数组来代替select中的bitmap，数组没有1024的限制，可以一次管理更多的cient。它和 select 的主要区别就是，去掉了 select只能监听 1024 个文件描述符的限制J

2、当pollfds数组中有事件发生，相应的revents置位为1，遍历的时候又置位回零，实现了pollfd数组的重用

**问题**

poll 解决了select缺点中的前两条，其本质原理还是select的方法，还存在select中原来的问题
1、pollfds数组拷贝到了内核态，仍然有开销
2、poll并没有通知用户态哪一个socket有数据,仍然需要O(n)的遍历

### epoll

**epoll的执行流程:**

1.当有数据的时候，会把相应的文件描述符“置位”，但是epool没有revent标志位，所以并不是真正的置位。这时候会把有数据的文件描述符放到队首。

2.epoll会返回有数据的文件描述符的个数

3.根据返回的个数 读取前N个文件描述符即可

4.读取、处理

![](C:\Users\sugon\Desktop\study_note\images\2023-02-14-16-28-16-image.png)

**三步调用**

![](C:\Users\sugon\Desktop\study_note\images\2023-02-14-16-19-11-image.png)

小总结

![](C:\Users\sugon\Desktop\study_note\images\2023-02-14-16-29-32-image.png)

## 总结

![](C:\Users\sugon\Desktop\study_note\images\2023-02-14-15-13-22-image.png)
